{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ce9f4a84-b600-4a9e-b8fb-2dea315ff31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "\n",
    "filename = \"aesop.txt\"\n",
    "\n",
    "with open(filename, encoding='utf-8-sig') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "seq_length = 20\n",
    "start_story = '|'*seq_length\n",
    "\n",
    "# CLEANUP\n",
    "text = text.lower()\n",
    "text = start_story + text\n",
    "text = text.replace('\\n\\n\\n\\n\\n', start_story)\n",
    "\n",
    "text = text.replace('\\n', ' ')\n",
    "text = re.sub(' +', '. ', text).strip()\n",
    "text = text.replace('..', '.')\n",
    "\n",
    "text = re.sub('([!\"#$%&()*+,-/:;<=>?@[\\]^_`{|}~])', r' \\1', text)\n",
    "text = re.sub('\\s{2,}', ' ', text)\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "tokenizer = Tokenizer(char_level = False, filters = '')\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "token_list = tokenizer.texts_to_sequences([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f28eb4f-494d-4e7f-a97e-0cdd62bb56e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | | | | | | | | | | | | | | | | | | | |the . fox . and . the . grapes . a . hungry . fox . saw . some . fine . bunches . of . grapes . hanging . from . a . vine . that . was . trained . along . a . h\n"
     ]
    }
   ],
   "source": [
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94d52ff1-d1fc-4855-bc2f-648810db3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def generate_sequences(token_list, step):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(0, len(token_list) - seq_length, step):\n",
    "        X.append(token_list[i: i + seq_length])\n",
    "        y.append(token_list[i + seq_length])\n",
    "    \n",
    "    y = np_utils.to_categorical(y, num_classes = total_words)\n",
    "    \n",
    "    num_seq = len(X)\n",
    "    print('Number of sequences:', num_seq, \"\\n\")\n",
    "    \n",
    "    return X, y, num_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66dca6d3-d2bc-46d7-b9e3-77cff9821f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 92889 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "seq_length = 20\n",
    "X, y, num_seq = generate_sequences(token_list, step)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ba8d9c5-42c8-4481-b1ff-c437599ba3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 3.0859\n",
      "Epoch 2/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8642\n",
      "Epoch 3/100\n",
      "2903/2903 [==============================] - 100s 34ms/step - loss: 2.7808\n",
      "Epoch 4/100\n",
      "2903/2903 [==============================] - 100s 35ms/step - loss: 2.8116\n",
      "Epoch 5/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.9500\n",
      "Epoch 6/100\n",
      "2903/2903 [==============================] - 97s 33ms/step - loss: 3.0765\n",
      "Epoch 7/100\n",
      "2903/2903 [==============================] - 97s 33ms/step - loss: 3.1462\n",
      "Epoch 8/100\n",
      "2903/2903 [==============================] - 97s 34ms/step - loss: 3.2026\n",
      "Epoch 9/100\n",
      "2903/2903 [==============================] - 98s 34ms/step - loss: 3.2454\n",
      "Epoch 10/100\n",
      "2903/2903 [==============================] - 98s 34ms/step - loss: 3.2541\n",
      "Epoch 11/100\n",
      "2903/2903 [==============================] - 744s 256ms/step - loss: 3.2567\n",
      "Epoch 12/100\n",
      "2903/2903 [==============================] - 100s 34ms/step - loss: 3.2667\n",
      "Epoch 13/100\n",
      "2903/2903 [==============================] - 101s 35ms/step - loss: 3.2584\n",
      "Epoch 14/100\n",
      "2903/2903 [==============================] - 101s 35ms/step - loss: 3.2654\n",
      "Epoch 15/100\n",
      "2903/2903 [==============================] - 109s 37ms/step - loss: 3.2633\n",
      "Epoch 16/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 3.2473\n",
      "Epoch 17/100\n",
      "2903/2903 [==============================] - 424s 146ms/step - loss: 3.2416\n",
      "Epoch 18/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 3.2119\n",
      "Epoch 19/100\n",
      "2903/2903 [==============================] - 106s 36ms/step - loss: 3.2014\n",
      "Epoch 20/100\n",
      "2903/2903 [==============================] - 108s 37ms/step - loss: 3.1713\n",
      "Epoch 21/100\n",
      "2903/2903 [==============================] - 107s 37ms/step - loss: 3.1427\n",
      "Epoch 22/100\n",
      "2903/2903 [==============================] - 105s 36ms/step - loss: 3.1341\n",
      "Epoch 23/100\n",
      "2903/2903 [==============================] - 105s 36ms/step - loss: 3.1076\n",
      "Epoch 24/100\n",
      "2903/2903 [==============================] - 104s 36ms/step - loss: 3.0902\n",
      "Epoch 25/100\n",
      "2903/2903 [==============================] - 104s 36ms/step - loss: 3.0673\n",
      "Epoch 26/100\n",
      "2903/2903 [==============================] - 104s 36ms/step - loss: 3.0463\n",
      "Epoch 27/100\n",
      "2903/2903 [==============================] - 104s 36ms/step - loss: 3.0227\n",
      "Epoch 28/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.9780\n",
      "Epoch 29/100\n",
      "2903/2903 [==============================] - 104s 36ms/step - loss: 2.9686\n",
      "Epoch 30/100\n",
      "2903/2903 [==============================] - 104s 36ms/step - loss: 2.9563\n",
      "Epoch 31/100\n",
      "2903/2903 [==============================] - 104s 36ms/step - loss: 2.9400\n",
      "Epoch 32/100\n",
      "2903/2903 [==============================] - 104s 36ms/step - loss: 2.9359\n",
      "Epoch 33/100\n",
      "2903/2903 [==============================] - 105s 36ms/step - loss: 2.9291\n",
      "Epoch 34/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.9208\n",
      "Epoch 35/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.9140\n",
      "Epoch 36/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.9048\n",
      "Epoch 37/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.9043\n",
      "Epoch 38/100\n",
      "2903/2903 [==============================] - 1004s 346ms/step - loss: 2.8965\n",
      "Epoch 39/100\n",
      "2903/2903 [==============================] - 976s 336ms/step - loss: 2.8922\n",
      "Epoch 40/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8947\n",
      "Epoch 41/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8923\n",
      "Epoch 42/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8938\n",
      "Epoch 43/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8864\n",
      "Epoch 44/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8690\n",
      "Epoch 45/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8772\n",
      "Epoch 46/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8576\n",
      "Epoch 47/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8543\n",
      "Epoch 48/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8638\n",
      "Epoch 49/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8570\n",
      "Epoch 50/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8471\n",
      "Epoch 51/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8525\n",
      "Epoch 52/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8460\n",
      "Epoch 53/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8705\n",
      "Epoch 54/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8548\n",
      "Epoch 55/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8540\n",
      "Epoch 56/100\n",
      "2903/2903 [==============================] - 103s 35ms/step - loss: 2.8605\n",
      "Epoch 57/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8540\n",
      "Epoch 58/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8735\n",
      "Epoch 59/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8702\n",
      "Epoch 60/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8629\n",
      "Epoch 61/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8840\n",
      "Epoch 62/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8826\n",
      "Epoch 63/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8694\n",
      "Epoch 64/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8588\n",
      "Epoch 65/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8575\n",
      "Epoch 66/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8568\n",
      "Epoch 67/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8433\n",
      "Epoch 68/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8349\n",
      "Epoch 69/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8293\n",
      "Epoch 70/100\n",
      "2903/2903 [==============================] - 103s 35ms/step - loss: 2.8296\n",
      "Epoch 71/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8487\n",
      "Epoch 72/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8420\n",
      "Epoch 73/100\n",
      "2903/2903 [==============================] - 103s 35ms/step - loss: 2.8479\n",
      "Epoch 74/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8545\n",
      "Epoch 75/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8673\n",
      "Epoch 76/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8821\n",
      "Epoch 77/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8844\n",
      "Epoch 78/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8595\n",
      "Epoch 79/100\n",
      "2903/2903 [==============================] - 103s 35ms/step - loss: 2.8536\n",
      "Epoch 80/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8595\n",
      "Epoch 81/100\n",
      "2903/2903 [==============================] - 103s 35ms/step - loss: 2.8349\n",
      "Epoch 82/100\n",
      "2903/2903 [==============================] - 103s 35ms/step - loss: 2.8371\n",
      "Epoch 83/100\n",
      "2903/2903 [==============================] - 103s 35ms/step - loss: 2.8301\n",
      "Epoch 84/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.8037\n",
      "Epoch 85/100\n",
      "2903/2903 [==============================] - 103s 35ms/step - loss: 2.8104\n",
      "Epoch 86/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.7932\n",
      "Epoch 87/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.7947\n",
      "Epoch 88/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.7670\n",
      "Epoch 89/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.7740\n",
      "Epoch 90/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.7704\n",
      "Epoch 91/100\n",
      "2903/2903 [==============================] - 102s 35ms/step - loss: 2.7745\n",
      "Epoch 92/100\n",
      "2903/2903 [==============================] - 103s 35ms/step - loss: 2.7635\n",
      "Epoch 93/100\n",
      "2903/2903 [==============================] - 103s 35ms/step - loss: 2.7531\n",
      "Epoch 94/100\n",
      "2903/2903 [==============================] - 103s 35ms/step - loss: 2.7419\n",
      "Epoch 95/100\n",
      "2903/2903 [==============================] - 103s 35ms/step - loss: 2.7510\n",
      "Epoch 96/100\n",
      "2903/2903 [==============================] - 103s 35ms/step - loss: 2.7380\n",
      "Epoch 97/100\n",
      "2903/2903 [==============================] - 103s 35ms/step - loss: 2.7502\n",
      "Epoch 98/100\n",
      "2903/2903 [==============================] - 103s 36ms/step - loss: 2.7376\n",
      "Epoch 99/100\n",
      "2903/2903 [==============================] - 103s 36ms/step - loss: 2.7209\n",
      "Epoch 100/100\n",
      "2903/2903 [==============================] - 103s 36ms/step - loss: 2.7312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff3752a5520>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, LSTM, Input, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "n_units = 256\n",
    "embedding_size = 100\n",
    "\n",
    "text_in = Input(shape = (None,))\n",
    "x = Embedding(total_words, embedding_size)(text_in)\n",
    "x = LSTM(n_units)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "text_out = Dense(total_words, activation = 'softmax')(x)\n",
    "\n",
    "model = Model(text_in, text_out)\n",
    "\n",
    "opti = RMSprop(learning_rate = 0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opti)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8410bcbf-ff78-457c-80c0-d0d6144d4743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temp(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len, temp):\n",
    "    output_text = seed_text\n",
    "    seed_text = start_story + seed_text\n",
    "    \n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences(seed_text)[0]\n",
    "        token_list = token_list[-max_sequence_len:]\n",
    "        token_list = np.reshape(token_list, (1, -1))\n",
    "\n",
    "        probs = model.predict(token_list, verbose=0)[0]\n",
    "        y_class = sample_with_temp(probs, temperature = temp)\n",
    "\n",
    "        output_word = tokenizer.index_word[y_class] if y_class > 0 else ''\n",
    "\n",
    "        if output_word == \"|\":\n",
    "            break\n",
    "\n",
    "        seed_text += output_word + ' '\n",
    "        output_text += output_word + ' '\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1f08c55a-98bc-45a0-acb4-63f4efcd90c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4893"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = model.predict([[60,50,90]], verbose=0)[0]\n",
    "len(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a3759f29-fd07-439f-b68e-4ae12db41dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4892\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e59c9a29-faef-43bc-9674-a02e56606b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'land turned fox long - but you him which for their boar fell for which one , show all : search fell .gutenberg this how home had all a one after : some own : you their you appeared mouse if when . added mercury .c will gave she the '"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('', 50, model, 400, 6.0)\n",
    "# tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a4881801-c458-4814-9759-6747d047599d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: aesop-model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: aesop-model/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7ff375a16a60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.save('aesop-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9cf787-cdc2-448d-8eaf-91a63faa36fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
