{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce9f4a84-b600-4a9e-b8fb-2dea315ff31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "\n",
    "filename = \"aesop.txt\"\n",
    "\n",
    "with open(filename, encoding='utf-8-sig') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "seq_length = 20\n",
    "start_story = '|'*seq_length\n",
    "\n",
    "# CLEANUP\n",
    "text = text.lower()\n",
    "text = start_story + text\n",
    "text = text.replace('\\n\\n\\n\\n\\n', start_story)\n",
    "\n",
    "text = text.replace('\\n', ' ')\n",
    "text = re.sub(' +', '. ', text).strip()\n",
    "text = text.replace('..', '.')\n",
    "\n",
    "text = re.sub('([!\"#$%&()*+,-/:;<=>?@[\\]^_`{|}~])', r' \\1', text)\n",
    "text = re.sub('\\s{2,}', ' ', text)\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "tokenizer = Tokenizer(char_level = False, filters = '')\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "token_list = tokenizer.texts_to_sequences([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f28eb4f-494d-4e7f-a97e-0cdd62bb56e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | | | | | | | | | | | | | | | | | | | |the . fox . and . the . grapes . a . hungry . fox . saw . some . fine . bunches . of . grapes . hanging . from . a . vine . that . was . trained . along . a . h\n"
     ]
    }
   ],
   "source": [
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94d52ff1-d1fc-4855-bc2f-648810db3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def generate_sequences(token_list, step):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(0, len(token_list) - seq_length, step):\n",
    "        X.append(token_list[i: i + seq_length])\n",
    "        y.append(token_list[i + seq_length])\n",
    "    \n",
    "    y = np_utils.to_categorical(y, num_classes = total_words)\n",
    "    \n",
    "    num_seq = len(X)\n",
    "    print('Number of sequences:', num_seq, \"\\n\")\n",
    "    \n",
    "    return X, y, num_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66dca6d3-d2bc-46d7-b9e3-77cff9821f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 92889 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "seq_length = 20\n",
    "X, y, num_seq = generate_sequences(token_list, step)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ba8d9c5-42c8-4481-b1ff-c437599ba3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2, 26])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8410bcbf-ff78-457c-80c0-d0d6144d4743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
